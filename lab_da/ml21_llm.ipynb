{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNnQ5+VXzMYeu3aNExmVk8q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JakeOh/202505_BD50/blob/main/lab_da/ml21_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM(Large Language Model, 거대 언어 모델)"
      ],
      "metadata": {
        "id": "N-kdNyrT6Hqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   시퀀스-시퀀스 작업(Sequence-to-Sequence)\n",
        "    *   시퀀스 데이터를 입력받아서 시퀀스 데이터를 출력하는 작업\n",
        "    *   자연어 처리(NLP, Natural Language Processing) 분야에서 요약, 번역 등의 작업 해당\n",
        "    *   두 개의 (순환) 신경망을 연결한 인코더-디코더(encoder-decoder) 구조가 널리 사용\n",
        "*   어텐션 메커니즘(Attention mechanism)\n",
        "    *   인코더-디코더 구조에서 사용된 순환 신경망의 성능을 향상시키기 위해서 고안\n",
        "    *   기존에는 인코더의 마지막 타임스텝에서 출력한 은닉 상태만을 사용해서 디코더가 새로운 텍스트를 생성\n",
        "    *   어텐션 메커니즘은 모든 타임스텝에서 인코더가 출력한 은닉 상태를 디코더가 참조할 수 있도록 고안\n",
        "    *   디코더가 새로운 토큰을 생성할 때 인코더가 처리한 토큰들 중에서 어떤 토큰에 주의(attention)를 기울일 지를 결정. (입력 토큰들마다 디코더가 중요도를 다르게 부여)\n",
        "*   트랜스포머 모델(Transformer model)\n",
        "    *   어텐션 메커니즘을 기반으로 하여 인코더-디코더 구조에서 순환층을 제거\n",
        "    *   인코더에서 한 번에 하나의 토크씩 처리하지 않고 입력 테스트 전체를 한 번에 처리\n",
        "    *   핵심 구성 요소\n",
        "        *   멀티 헤드 어텐션(multi-head attention)\n",
        "        *   층 정규화(layer normalization)\n",
        "        *   잔차 연결(residual connection)\n",
        "        *   피드 포워드 네트워크(feed-forward network)"
      ],
      "metadata": {
        "id": "ccV_u3Gn6OrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://pbs.twimg.com/media/GJg3QtMXwAAvUT4?format=jpg&name=large\"\n",
        "    alt=\"LLM 가계도\" />"
      ],
      "metadata": {
        "id": "lpSwQcdLAik9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Encoder 기반 모델\n",
        "    *   텍스트 (긍정/부정) 분류\n",
        "    *   개체명 인식 - 텍스트에서 사람 이름, 지역 이름, 회사 이름 등의 고유 명사를 식별.\n",
        "    *   BERT, RoBERTa, ...\n",
        "*   Encoder-Decoder 기반 모델\n",
        "    *   문서 요약, 번역, 질문-답변(질문과 문맥 텍스트가 주어졌을 때 문맥 안에서 답을 찾아서 생성)\n",
        "    *   T5, BART, ...\n",
        "*   Decoder 기반 모델\n",
        "    *   텍스트 생성 - 챗봇, 질문 답변, 요약, 번역\n",
        "    *   디코더:\n",
        "        *   이전까지 생성한 텍스트를 입력받아 다음 토큰을 예측하는 방식\n",
        "        *   인코더로부터의 입력이 없으면 디코더는 아무것도 생성할 수 없음.\n",
        "        *   이전에 생성한 텍스트인 것처럼 어떤 텍스트를 입력해 주면 인코더 도움 없이 다음 토큰을 예측.\n",
        "        *   프롬프트(prompt): 이전에 생성한 텍스트인 것처럼 전달하는 초기 텍스트.\n",
        "    *   GPT-4, GPT-5, LLaMA, ...\n",
        "    *   현재 가장 활발히 연구되고 있는 LLM 분야."
      ],
      "metadata": {
        "id": "sda7uyojGsLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KoBART 모델로 문서 요약하기"
      ],
      "metadata": {
        "id": "xHlMq835MqdN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uNc23VAu6Dvk"
      },
      "outputs": [],
      "source": [
        "import transformers  # HuggingFace의 transformers 기반 모델을 사용하기 위한 패키지"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HuggingFace에서 오픈 소스로 공개된 모델을 다운로드해서 로컬에서 실행할 수 있도록 해줌.\n",
        "pipe = transformers.pipeline(task=\"summarization\",  # 문서 요약 작업\n",
        "                             model=\"sshleifer/distilbart-cnn-12-6\",\n",
        "                             device=0)\n",
        "# device=0: GPU 사용, device=-1(기본값): CPU"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ULsqrEDMNDe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(pipe)  # Pipeline 클래스 객체"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "collapsed": true,
        "id": "tSGC2W2MT-3B",
        "outputId": "d0100253-eb57-47e8-c639-cd4df7a249d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.pipelines.text2text_generation.SummarizationPipeline"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>transformers.pipelines.text2text_generation.SummarizationPipeline</b><br/>def __call__(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text2text_generation.py</a>Summarize news articles and other documents.\n",
              "\n",
              "This summarizing pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
              "`&quot;summarization&quot;`.\n",
              "\n",
              "The models that this pipeline can use are models that have been fine-tuned on a summarization task, which is\n",
              "currently, &#x27;*bart-large-cnn*&#x27;, &#x27;*google-t5/t5-small*&#x27;, &#x27;*google-t5/t5-base*&#x27;, &#x27;*google-t5/t5-large*&#x27;, &#x27;*google-t5/t5-3b*&#x27;, &#x27;*google-t5/t5-11b*&#x27;. See the up-to-date\n",
              "list of available models on [huggingface.co/models](https://huggingface.co/models?filter=summarization). For a list\n",
              "of available parameters, see the [following\n",
              "documentation](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)\n",
              "\n",
              "Unless the model you&#x27;re using explicitly sets these generation parameters in its configuration files\n",
              "(`generation_config.json`), the following default values will be used:\n",
              "- max_new_tokens: 256\n",
              "- num_beams: 4\n",
              "\n",
              "Usage:\n",
              "\n",
              "```python\n",
              "# use bart in pytorch\n",
              "summarizer = pipeline(&quot;summarization&quot;)\n",
              "summarizer(&quot;An apple a day, keeps the doctor away&quot;, min_length=5, max_length=20)\n",
              "\n",
              "# use t5 in tf\n",
              "summarizer = pipeline(&quot;summarization&quot;, model=&quot;google-t5/t5-base&quot;, tokenizer=&quot;google-t5/t5-base&quot;, framework=&quot;tf&quot;)\n",
              "summarizer(&quot;An apple a day, keeps the doctor away&quot;, min_length=5, max_length=20)\n",
              "```\n",
              "Arguments:\n",
              "    model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
              "        The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
              "        [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
              "    tokenizer ([`PreTrainedTokenizer`]):\n",
              "        The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
              "        [`PreTrainedTokenizer`].\n",
              "    modelcard (`str` or [`ModelCard`], *optional*):\n",
              "        Model card attributed to the model for this pipeline.\n",
              "    framework (`str`, *optional*):\n",
              "        The framework to use, either `&quot;pt&quot;` for PyTorch or `&quot;tf&quot;` for TensorFlow. The specified framework must be\n",
              "        installed.\n",
              "\n",
              "        If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
              "        both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
              "        provided.\n",
              "    task (`str`, defaults to `&quot;&quot;`):\n",
              "        A task-identifier for the pipeline.\n",
              "    num_workers (`int`, *optional*, defaults to 8):\n",
              "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
              "        workers to be used.\n",
              "    batch_size (`int`, *optional*, defaults to 1):\n",
              "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
              "        the batch to use, for inference this is not always beneficial, please read [Batching with\n",
              "        pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
              "    args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
              "        Reference to the object in charge of parsing supplied pipeline parameters.\n",
              "    device (`int`, *optional*, defaults to -1):\n",
              "        Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
              "        the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
              "    torch_dtype (`str` or `torch.dtype`, *optional*):\n",
              "        Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
              "        (`torch.float16`, `torch.bfloat16`, ... or `&quot;auto&quot;`)\n",
              "    binary_output (`bool`, *optional*, defaults to `False`):\n",
              "        Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
              "        the raw output data e.g. text.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 245);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \\\n",
        "'''\n",
        "During the 1905–06 English football season, New Brompton F.C. competed in the Southern League Division One. It was the 12th season in which the club competed in the Southern League and the 11th in Division One. The team began the season in poor form; they failed to score any goals in six of their first eight Southern League games. By the midpoint of the season, the team had won only three times and were close to the bottom of the league table. The team's form improved in the new year, with three wins in the first seven Southern League games of 1906, but they ended the season in similar fashion to how they had started it, failing to score in eight of the final nine league games. New Brompton finished the season in 17th place out of 18 teams in the division.\n",
        "\n",
        "New Brompton also competed in the FA Cup, reaching the second round. The team played a total of 37 league and cup matches, winning 8, drawing 9 and losing 20. Bill Marriott was the club's top goalscorer, with four goals in the Southern League and one in the FA Cup; this figure was the lowest to date with which a player had finished a season as New Brompton's top scorer. Joe Walton made the most appearances, playing in 36 of the team's 37 competitive games. The highest attendance recorded at the club's home ground, Priestfield Road, was 5,500 for a game against Portsmouth on 27 January 1906.\n",
        "\n",
        "The club's first match of the season, on 2 September, was away to Queens Park Rangers; Floyd, Phillips, Sheridan and Marriott all made their Southern League debuts for New Brompton.[13] Queens Park Rangers scored two goals in each half to win 4–0; the correspondent for The Daily News stated that the home team were \"the better side at every point\" and well deserved their large victory.[14] The first Southern League game of the season at New Brompton's ground, Priestfield Road, a week later against Bristol Rovers, resulted in a 3–0 win for the away team.[13][15] Phillips scored New Brompton's first Southern League goal of the season on 16 September and the team achieved their first victory, beating Northampton Town 2–0,[13][16] before losing again in their next game, a 4–0 defeat away to Portsmouth.[17] After a goalless draw with Swindon Town, New Brompton were 15th out of 18 teams in the Division One table at the end of September.[18]\n",
        "'''"
      ],
      "metadata": {
        "id": "icL3RHp6UuAv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline 객체를 함수처럼 호출할 수 있음.\n",
        "result = pipe(sample_text)"
      ],
      "metadata": {
        "id": "Rv1ZqarOULsh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(result)  # dict를 원소로 갖는 리스트(list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3k9-8JhVvXyQ",
        "outputId": "53c39678-3d64-4d31-b560-d0ea11adfbfd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 리스트의 첫번째 원소 dict에서 키 summary_text에 해당하는 값을 출력\n",
        "print(result[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeMfykYLVrAH",
        "outputId": "aaaf6adb-bd42-40b0-d902-c49ecac8f40f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " During the 1905–06 English football season, New Brompton F.C. competed in the Southern League Division One . The team played 37 league and cup matches, winning 8, drawing 9 and losing 20 . The highest attendance recorded at the club's home ground, Priestfield Road, was 5,500 for a game against Portsmouth in January 1906 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text_kor = \\\n",
        "'''\n",
        "1896년 하계 올림픽(영어: 1896 Summer Olympics, Games of the I Olympiad, 그리스어: Θερινοί Ολυμπιακοί Αγώνες 1896)은 393년을 마지막으로 끝난 고대 올림픽 대회 이후 열린 첫 근대 올림픽 대회다. 1896년 4월 6일부터 4월 15일까지 그리스 아테네에서 개최되었다. 고대 그리스가 올림픽의 발상지여서 첫 근대 올림픽이 열리기에 적당한 장소였던 아테네는 1894년 6월 23일에 파리에서, 프랑스의 역사학자인 쿠베르탱이 주관한 올림픽 의회에서 만장일치로 개최지 자격을 얻었다. 또한 하계 올림픽이 진행되는 동안 국제 올림픽 위원회(IOC)가 조직되었다.\n",
        "\n",
        "여러 어려움을 이겨낸 1회 올림픽은 성공적인 평가를 받았다. 당시의 국제 경기 중에서는 이 대회가 가장 많은 국제적인 참여를 이끌어냈다. 19세기 때 유일한 올림픽 경기장으로 쓰인 파나티나이코 경기장은 경기를 보려는 사람들이 몰려서 인산인해를 이루었다.[2] 개최국인 그리스에게 있어서 가장 절정이었던 부분은 마라톤 경기에서 자국 선수인 스피리돈 루이스가 승리의 영광을 차지한 것이었다. 이 대회에서는 카를 슈만이 레슬링과 체조에서 4개의 금메달을 따서 가장 많은 금메달을 획득했다. 가장 많이 메달을 딴 선수는 헤르만 바인게르트너로 6개의 메달을 땄다.\n",
        "\n",
        "대회가 끝난 후 IOC는 이후의 올림픽을 계속 그리스에서 개최할 것인가를 놓고 피에르 드 쿠베르탱 남작과 그리스 임금인 요르요스 1세, 몇몇 미국 선수들 사이에서 갈등이 있었으나 1900년 대회가 이미 파리에서 열리기로 결정된 상태였고, 이후 올림픽은 세계를 순환하면서 개최하게 된다. 1906년 중간 올림픽을 제외하고는 그리스는 2004년 하계 올림픽을 개최할 때까지 108년간 올림픽을 개최하지 못했다.\n",
        "'''"
      ],
      "metadata": {
        "id": "A7BdlGDUYqTX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pipe(sample_text_kor)\n",
        "# 해당 모델은 한글 텍스트로는 훈련이 되지 않아서 요약 작업을 실행할 수 없음."
      ],
      "metadata": {
        "id": "hqLVDGSfYxsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_kor = transformers.pipeline(task='summarization',\n",
        "                                 model='EbanLee/kobart-summary-v3')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "shdyy9_VZiiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_kor(sample_text_kor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WunzID-ZZq_X",
        "outputId": "e27fbe1b-a8c3-42de-98c7-d2c586ccbd08"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': '1896년 하계 올림픽은 고대 올림픽 대회 이후 열린 첫 근대 올림픽 대회로 아테네에서 개최되었다.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline 객체의 model 속성(property)의 config 속성(property)\n",
        "print(pipe_kor.model.config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqlbU-sHw4XO",
        "outputId": "0411ce97-b148-4bee-a63e-5a306872cd7f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BartConfig {\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"author\": \"EbanLee(rudwo6769@gmail.com)\",\n",
            "  \"bos_token_id\": 1,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.1,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 1,\n",
            "  \"do_blenderbot_90_layernorm\": false,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"extra_pos_embeddings\": 2,\n",
            "  \"force_bos_token_to_be_generated\": false,\n",
            "  \"forced_eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"NEGATIVE\",\n",
            "    \"1\": \"POSITIVE\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"kobart_version\": 2.0,\n",
            "  \"label2id\": {\n",
            "    \"NEGATIVE\": 0,\n",
            "    \"POSITIVE\": 1\n",
            "  },\n",
            "  \"max_position_embeddings\": 1026,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"scale_embedding\": false,\n",
            "  \"static_position_embeddings\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 300,\n",
            "      \"min_length\": 12,\n",
            "      \"no_repeat_ngram_size\": 15,\n",
            "      \"num_beams\": 6,\n",
            "      \"repetition_penalty\": 1.5\n",
            "    }\n",
            "  },\n",
            "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.55.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "토크나이저(tokenizer): 텍스트를 토큰(정수)으로 변환하는 객체."
      ],
      "metadata": {
        "id": "wL8aklkKx_nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipe_kor.tokenizer.vocab_size)  # tokenizer가 사용하는 어휘 사전의 크기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVBJFj9QxrpG",
        "outputId": "28c265ec-7fe0-4d52-b0b9-784769cf35be"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pipeline 함수는 LLM 모델과 텍스트를 토큰화하는 tokenizer 객체를 함께 다운로드함.\n",
        "\n",
        "LLM 모델이 문서 요약을 하기 전에 입력 텍스트를 tokenizer를 사용해서 전처리를 하고, 모델이 예측을 수행."
      ],
      "metadata": {
        "id": "-F0AuvZgyx9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = pipe_kor.tokenizer.vocab  # 어휘 사전 - dict(단어를 키로 하고 토큰을 값으로 하는)"
      ],
      "metadata": {
        "id": "lubvYIEFzL_e"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(vocab.items())[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uswiBKLVzUwd",
        "outputId": "de1a2578-e7e4-456b-a087-ff855b0ff92e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('쓇', 11647),\n",
              " ('어리', 27108),\n",
              " ('▁3번', 28391),\n",
              " ('둥', 9935),\n",
              " ('裹', 7373),\n",
              " ('歙', 4895),\n",
              " ('▁‘201', 22810),\n",
              " ('▁벌어진', 21092),\n",
              " ('▁재벌', 18954),\n",
              " ('블', 11011)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenizer 객체의 메서드"
      ],
      "metadata": {
        "id": "FGrDHzNW0PKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = pipe_kor.tokenizer"
      ],
      "metadata": {
        "id": "41PJoIdy0jj9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize(): 텍스트 --> 토큰들의 리스트\n",
        "tokens = tokenizer.tokenize('혼자 공부하는 머신러닝 딥러닝')"
      ],
      "metadata": {
        "id": "VL1gmr_X0qVc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFcQ8paX09Mk",
        "outputId": "cd704d70-b575-4846-a17c-00c395f4f958"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁혼자', '▁공부', '하는', '▁머', '신', '러', '닝', '▁', '딥', '러', '닝']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert_tokens_to_ids(): 토큰들의 리스트를 토큰에 해당하는 정수들의 리스트로 변환\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)"
      ],
      "metadata": {
        "id": "tDOmdG1V0-M0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFJ2saCn1q2E",
        "outputId": "40b0a355-433f-422b-f218-e00dc9323579"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16814, 16962, 14049, 14771, 11467, 10277, 9747, 1700, 10021, 10277, 9747]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode(): tokenize + convert_tokens_to_ids\n",
        "encoded = tokenizer.encode('혼자 공부하는 머신러닝 딥러닝')"
      ],
      "metadata": {
        "id": "IRrO5scG1rvc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NuyACgh2IIb",
        "outputId": "1ae0b098-6d74-4140-a8b7-a02cb6011081"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 16814, 16962, 14049, 14771, 11467, 10277, 9747, 1700, 10021, 10277, 9747, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded2 = tokenizer.encode('KoBART 모델을 사용한 문서 요약')\n",
        "print(encoded2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIHUUOKX2b87",
        "outputId": "f73bc65a-cecf-43b1-9389-6eddf05c702c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 14572, 310, 265, 264, 281, 283, 24224, 21032, 26052, 26200, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert_ids_to_tokens(): 토큰 아이디들의 리스트를 토큰들의 리스트로 변환\n",
        "tokens2 = tokenizer.convert_ids_to_tokens(encoded2)\n",
        "print(tokens2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U18c7qyJ3WCb",
        "outputId": "65271c02-6fdf-4452-dfe5-be0f35c84d2b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', '▁K', 'o', 'B', 'A', 'R', 'T', '▁모델을', '▁사용한', '▁문서', '▁요약', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decode(): 토큰 아이디들을 갖는 리스트를 텍스트로 변환\n",
        "tokenizer.decode(encoded2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uu1ORtNm34Yb",
        "outputId": "a58e7580-c947-4e30-d1ac-043180653948"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> KoBART 모델을 사용한 문서 요약</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(encoded2[1:-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5eYpcGSp4NIa",
        "outputId": "965e23f8-3c8f-4091-db39-07ad873b2740"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KoBART 모델을 사용한 문서 요약'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline 동작 원리\n",
        "\n",
        "텍스트 입력 --> tokenizer.encode() --> LLM 모델 --> tokenizer.decode()"
      ],
      "metadata": {
        "id": "QFeeW9HT4ZfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXAONE-3.5 모델로 텍스트 생성하기"
      ],
      "metadata": {
        "id": "74cREhPA91Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXAONE-3.5 모델에서 사용하는 토크나이저 객체를 로딩\n",
        "exaone_tokenizer = \\\n",
        "    transformers.AutoTokenizer.from_pretrained('LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct')"
      ],
      "metadata": {
        "id": "GfTIJ3QVAUEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXAONE-3.5 Pipeline 객체 생성\n",
        "exaone = transformers.pipeline(task='text-generation',\n",
        "                               model='LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct',\n",
        "                               tokenizer=exaone_tokenizer,\n",
        "                            #    device=0,\n",
        "                               trust_remote_code=True)"
      ],
      "metadata": {
        "id": "SLD3sfCPAvUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 메시지 템플릿: 'role'과 'content' 키를 갖는 dict를 원소로 갖는 리스트(list)\n",
        "# role: 역할(system, user, assitant)\n",
        "# content: 내용\n",
        "messages = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': '''너는 쇼핑몰 홈페이지에 올라온 Q&A에 답변을 하는 챗봇이야.\n",
        "                확정적인 답변을 하지 말고 제품 담당자가 정확한 답변을 하기 위해 시간이\n",
        "                필요하다는 간단하고 친절한 답변을 생성해줘.'''\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': '이 다이어리에는 내년도 공휴일 정보가 있나요?'\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "T1IiRSAWCLp3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exaone(messages, max_new_tokens=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7RXP1HpEkSb",
        "outputId": "a93a4ab7-f822-4f3a-fd5a-02a23d3bb201"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': [{'role': 'system',\n",
              "    'content': '너는 쇼핑몰 홈페이지에 올라온 Q&A에 답변을 하는 챗봇이야.\\n                확정적인 답변을 하지 말고 제품 담당자가 정확한 답변을 하기 위해 시간이 \\n                필요하다는 간단하고 친절한 답변을 생성해줘.'},\n",
              "   {'role': 'user', 'content': '이 다이어리에는 내년도 공휴일 정보가 있나요?'},\n",
              "   {'role': 'assistant',\n",
              "    'content': '죄송하지만, 현재 제가 실시간으로 각 국가의 공휴일 정보를 업데이트 받는 기능은 없습니다. 정확한 내년도 공휴일 정보는 한국의 공식 공휴일 발표나 신뢰할 수 있는 정부 웹사이트를 통해 확인하시는 것이 가장 정확할 거예요. 제품 담당자분께 문의하시면 도움을 드릴 수 있을 것 같습니다. 감사합니다!'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}